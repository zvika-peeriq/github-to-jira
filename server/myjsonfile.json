{"projects":[{"name":"Analytics","key":"AN","issues":[{"summary":"OWS: CPR is showing NAs and negative values for 0 MOB in the OWS: All pool","externalId":"2965","repo":"aqueduct","description":"CPR at month 0 in the OWS: All pool is showing -48.14%.\r\n\r\n![image](https://user-images.githubusercontent.com/55464562/130520656-73f6d4bd-1630-4d5a-aa55-451f6601fa09.png)\r\n\r\nWhen stratifying by month vintage, I see NAs at month 0\r\n\r\n![image](https://user-images.githubusercontent.com/55464562/130520242-1f5388d7-30e1-4f5b-b2e5-70c9a0658620.png)\r\n\r\nAccording to Jay, there shouldn't be any negative unscheduled principal values:\r\n\r\n![image](https://user-images.githubusercontent.com/55464562/130520743-637f12e9-2b59-47a8-87b1-404f0a5c726f.png)\r\n\r\n","created":"2021-08-23T21:18:42.000Z","issueType":"Task","reporter":"Tito Donis","status":"TO DO","labels":["[zube]: Backlog","data issue","client: ows","jira"],"comments":[]},{"summary":"OWS: Ever Delinquent $ metric fluctuates over time","externalId":"2957","repo":"aqueduct","description":"OWS wants to track of loans that have ever been DQ. By definition, the Ever Delinquent metric should capture the balance for loans that have ever been delinquent, but rather than the amount getting higher, it fluctuates. \r\n\r\nSee the drop in ever DQ from 8/4 to 8/5. in the screenshot below The EOP and loan count do not drop in the OWS QLN pool:\r\n\r\n![image](https://user-images.githubusercontent.com/55464562/129636684-f15c35d5-1f4c-41a8-8956-93ebd6c1de0e.png)\r\n","created":"2021-08-16T22:27:05.000Z","issueType":"Task","reporter":"Tito Donis","status":"TO DO","labels":["[zube]: Backlog","bug","data issue","client: ows","jira"],"comments":[]},{"summary":"need ETL jobs scheduled immediately after files come in","externalId":"2942","repo":"aqueduct","description":"there is a lambda defined in `data-availability-service` that writes to kafka when s3 files come in - and there is a kafka listener in airflow\r\n\r\nso,\r\n1) are those kafka messages actually getting picked up by airflow?\r\n2) if so, we want to use sqs instead of kafka\r\n3) if not, implement a working lambda to sqs to airflow system\r\n4) ensure airflow schedules job when the appropriate queue messages are read in by airflow (and that lambda should be writing the right message when the right s3 files come in and airflow should know given certain messages, whether it's time to schedule the job or not)\r\n\r\nmost of this should be there in some form, just need to confirm those 4 things and fill in what's missing","created":"2021-08-12T22:31:10.000Z","issueType":"Task","reporter":"Barry Ickow","status":"TO DO","labels":["[zube]: Backlog","jira"],"comments":[]},{"summary":"ETL wishlist","externalId":"2928","repo":"aqueduct","description":"1) simplify airflow (do we need glusterfs? etc.) and get it working again in general of course\r\n\r\n2) FIFO queues for loan id queue - this should prevent the need to address duplicate processed loan records downstream (currently in the copy / load table task) as FIFO SQS should guarantee exact-once delivery\r\n\r\n3) Remove the trigger queue to AJO - a REST request to AJO should be simpler with no downsides (but we will confirm , along with testing)\r\n\r\n4) Confirm if ES needs to pull from a queue of messages or if writing directly to ES is fine (which would be much simpler) - also see other prior git issues about auto-pruning ES (time based indices, etc.)\r\n\r\n5) status service should not be inferring status from pulling and aggregating certain log messages in ES - something like status should be using something much more structured like postgres or at least something like redis - not searching and aggregating log messages in ES\r\n\r\n6) streamline the terminator service - some of the above should allow us to delete some of terminator - and we should be leveraging configuring infrastructure in AWS (like SQS queues possibly) to automatically be removed after x number of minutes of inactivity, where possible (instead of having our terminator service)\r\n\r\n7) find a managed service version of Vertica - and potentially use instead of RedShift as well","created":"2021-07-29T15:47:01.000Z","issueType":"Task","reporter":"Barry Ickow","status":"TO DO","labels":["[zube]: Backlog","jira"],"comments":[{"author":"Barry Ickow","body":"8) csv2cassandra probably doesn't scale well - need the csv2cassandra instances to auto-scale so many runs can run simultaneously (use ECS or Fargate, etc. instead of ElasticBeanstalk potentially)","created":"2021-07-29T15:53:26.000Z"},{"author":"Jay Patel","body":"@barryPIQ good list! Few things I want to point out/add.\r\n\r\n9) It looks like email alerts are down for ETL jobs, we def need to get that back up. Something with a SMTP Auth error. Also maybe looking into sending success email alerts when a job completes.\r\n\r\n-I do have some specific requests surrounding airflow I want to address once we start exploring how to want to use it. Given some of our clients/potentially future ones are time sensitive i do want to see if theres a way to trigger multiple runs in a day without risk of running etl jobs multiple times. But we can discuss this + others once we start looking into it","created":"2021-07-29T18:22:54.000Z"},{"author":"Barry Ickow","body":"thanks @jayp0413 \r\n\r\nand from this morning 10) need much better error messaging at least in our logs for errors that cause loans to drop, even if the root cause is a source data issue - root cause should be in the error message","created":"2021-08-02T16:42:58.000Z"},{"author":"Barry Ickow","body":"11) why does the django task_manager manage.py in iris have ~100 calls to MDS every time manage.py is run?\r\n\r\n12) get all the DQ rules out of SQL and into Batch (python), and then get EMR into Batch (downstream of DQ rules)","created":"2021-08-19T17:27:53.000Z"},{"author":"Barry Ickow","body":"13) remove un-necessary python packages (if any) and upgrade existing\r\n\r\n14) re-visit docker-compose files for running locally","created":"2021-08-20T14:37:21.000Z"},{"author":"Zvika Badalov","body":"Test: \r\n![image](https://user-images.githubusercontent.com/83380785/130889840-06840cad-0d15-444f-a786-3e4dd0135f39.png)\r\n","created":"2021-08-26T02:22:11.000Z"}]},{"summary":"Need a DQ rule (or anything else) which forces an ETL workflow to fail if processed loan count does not match the loan count in cassandra","externalId":"2927","repo":"aqueduct","description":null,"created":"2021-07-29T15:35:11.000Z","issueType":"Task","reporter":"Barry Ickow","status":"TO DO","labels":["[zube]: Backlog","jira"],"comments":[{"author":"Barry Ickow","body":"batch should write loan-id, run-id, error/warning message for any loans legitimately dropped to a database, and the dq rule should confirm cassandra count = redshift count + error/warning count","created":"2021-08-05T16:13:46.000Z"}]},{"summary":"Fix Cassandra space issues","externalId":"2926","repo":"aqueduct","description":"The hypothesis is that the high space usage on Cassandra is affecting Cassandra operations, most importantly causing not all loans to be streamed out of Cassandra.  Hypothesized this due to more Cassandra errors recently (load errors, NullPointer errors retrieving data) and more data in Cassandra recently.\r\n\r\n@jayp0413 has just cleared Public LC and Prosper data to free some space, @Zvika-Badalov has just kicked off the cassandra cleanup maintenance utility, and @Matt-Aquiles is working on the scala upgrades necessary to integrate AWS's managed cassandra so these things are prevented , or at least easier to prevent.\r\n\r\nWe will look to see if the space clearing on current cassandra results in subsequent runs having full loan counts to confirm our hypothesis though.","created":"2021-07-29T15:33:49.000Z","issueType":"Task","reporter":"Barry Ickow","status":"TO DO","labels":["[zube]: DevStarted","jira"],"comments":[{"author":"Barry Ickow","body":"just waiting on the cassandra cleanup utility to complete and we'll monitor subsequent runs","created":"2021-07-29T15:48:58.000Z"},{"author":"Barry Ickow","body":"along with checking space cleared after the cassandra cleanup completes","created":"2021-07-29T15:50:35.000Z"}]},{"summary":"csv-to-cassandra loading errors","externalId":"2924","repo":"aqueduct","description":"Will edit with more details but seems to be 2 separate issues:\r\n\r\n1. csv-to-cassandra not working at all(loading,deleting, anything). Seems to have been due to a space issue with cassdandra. @Zvika-Badalov @barryPIQ plan to run cleanup jobs on the cassandra clusters when we are in a good spot with data loads.\r\n\r\n2. csv-to-cassandra consistently errors out when loading files. Usually works after many attempts of deleting and reloading files but not sure of the cause here.(could also be related to the first issue)","created":"2021-07-26T16:38:02.000Z","issueType":"Task","reporter":"Jay Patel","status":"TO DO","labels":["[zube]: DevStarted","service: csv-to-cassandra","jira"],"comments":[]},{"summary":"Airflow Fix + Revamp","externalId":"2923","repo":"aqueduct","description":"Details to follow","created":"2021-07-26T16:33:32.000Z","issueType":"Task","reporter":"Jay Patel","status":"TO DO","labels":["[zube]: DevStarted","service: airflow","jira"],"comments":[{"author":"Barry Ickow","body":"1) try upgrading airflow to newer version\r\n2) does that get logs back on glusterfs? if not, can we use S3 / postgres to replace glusterfs entirely\r\n3) does definition of \"file exists\" account for file being fully loaded?\r\n4) does airflow need to listen to a queue to accept the schedule request from lambda, as opposed to lambda just sending a rest request to airflow when necessary files are in? if the former, at least replace kafka with SQS\r\n5) cleanup - once lambda is getting airflow to automatically schedule immediately, no need for the 9am preset airflow scehdule (or wtvr times are currently preset) ","created":"2021-08-16T17:00:40.000Z"}]},{"summary":"[Reporting] OWS - Cash Settlement Report","externalId":"2922","repo":"aqueduct","description":"## Context\r\nSet of three reports requested from Nick/OWS for reporting. Priority is (bold is current issue):\r\n1. Aug Sprint 2 (ending 8/28): Settlement Email\r\n2. Sep Sprint 1 (ending 9/11): Trade Report\r\n3. **Sep Sprint 2 (ending 10/2): Cash Settlement Report**\r\n\r\n## Feature Description\r\nAutomatically generate the Cash Settlement Report\r\n\r\n## Test Cases\r\n- [ ] Confirm data exists\r\n- [ ] Confirm report format matches exactly with example in mockup\r\n- [ ] Confirm data in report matches with queried data\r\n\r\n## Additional Notes\r\nBRD and mock up below:\r\n[OWS - Cash Settlement Report.docx](https://zube.io/files/peeriq/ef99721d55b2f36ecb3d8d098856903f-ows-cash-settlement-report.docx)\r\n[OWS - Cash Settlement Report - Example.xlsx](https://zube.io/files/peeriq/349abc8cee04ccd3e144180809a64477-ows-cash-settlement-report-example.xlsx)","created":"2021-07-26T15:23:49.000Z","issueType":"Task","reporter":"Peter Cao","status":"TO DO","labels":["[zube]: Backlog","type: reporting","client: ows","jira"],"comments":[]},{"summary":"[Reporting] OWS - Trade Report","externalId":"2921","repo":"aqueduct","description":"## Context\r\nSet of three reports requested from Nick/OWS for reporting. Priority is (bold is current issue):\r\n1. Aug Sprint 2 (ending 8/28): Settlement Email\r\n2. **Sep Sprint 1 (ending 9/11): Trade Report**\r\n3. Sep Sprint 2 (ending 10/2): Cash Settlement Report\r\n\r\n## Feature Description\r\nGenerate the trade report for OWS automatically and on a daily basis\r\n\r\n## Test Cases\r\n- [ ] Confirm data exists\r\n- [ ] Confirm report format matches exactly with example in mockup\r\n- [ ] Confirm data in report matches with queried data\r\n\r\n## Additional Notes\r\nBRD and mock up below:\r\n[OWS - Trade Report - Affirm Trade Example.csv](https://zube.io/files/peeriq/84842b64ca76edbd3e0ff220ff4ff8a0-ows-trade-report-affirm-trade-example.csv)\r\n[OWS - Trade Report.docx](https://zube.io/files/peeriq/bd64bd625501ed1baf51ab00a1d6647e-ows-trade-report.docx)","created":"2021-07-26T15:21:58.000Z","issueType":"Task","reporter":"Peter Cao","status":"TO DO","labels":["[zube]: Backlog","type: reporting","client: ows","jira"],"comments":[]},{"summary":"[Reporting] OWS - Settlement Email","externalId":"2920","repo":"aqueduct","description":"## Context\r\nSet of three reports requested from Nick/OWS for reporting. Priority is (bold is current issue):\r\n1. **Aug Sprint 2 (ending 8/28): Settlement Email**\r\n2. Sep Sprint 1 (ending 9/11): Trade Report\r\n3. Sep Sprint 2 (ending 10/2): Cash Settlement Report\r\n\r\n## Feature Description\r\nAutomatically generate and populate a preset settlement report email that goes out to OWS/Nick on a daily basis\r\n\r\n## Test Cases\r\n- [ ] Confirm data exists\r\n- [ ] Confirm report format matches exactly with example in BRD\r\n- [ ] Confirm data in report matches with queried data\r\n\r\n## Additional Notes\r\nBRD below:\r\n[OWS - Settlement Email.docx](https://zube.io/files/peeriq/9ddb975a6317e6260ede84e1d157d6df-ows-settlement-email.docx)","created":"2021-07-26T15:19:12.000Z","issueType":"Task","reporter":"Peter Cao","status":"TO DO","labels":["[zube]: Backlog","type: reporting","client: ows","jira"],"comments":[]},{"summary":"[Reporting] OWS Reporting Data Infrastructure Setup","externalId":"2919","repo":"aqueduct","description":"## Context\r\nOWS Reporting Data Infrastructure Setup for Nick/OWS asks for subsequent Reporting asks\r\n\r\n## Feature Description\r\nSet up data infrastructure to meeting reporting asks\r\n\r\n## Test Cases\r\nConfirm data infrastructure scope meetings reporting requirements\r\n\r\n## Additional Notes\r\nSee attached BRDs and associated mock ups attached\r\nBRDs and examples:\r\n[OWS - Cash Settlement Report.docx](https://zube.io/files/peeriq/ef99721d55b2f36ecb3d8d098856903f-ows-cash-settlement-report.docx)\r\n[OWS - Cash Settlement Report - Example.xlsx](https://zube.io/files/peeriq/349abc8cee04ccd3e144180809a64477-ows-cash-settlement-report-example.xlsx)\r\n\r\n[OWS - Settlement Email.docx](https://zube.io/files/peeriq/9ddb975a6317e6260ede84e1d157d6df-ows-settlement-email.docx)\r\n\r\n[OWS - Trade Report.docx](https://zube.io/files/peeriq/bd64bd625501ed1baf51ab00a1d6647e-ows-trade-report.docx)\r\n[OWS - Trade Report - Affirm Trade Example.csv](https://zube.io/files/peeriq/84842b64ca76edbd3e0ff220ff4ff8a0-ows-trade-report-affirm-trade-example.csv)","created":"2021-07-26T15:10:03.000Z","issueType":"Task","reporter":"Peter Cao","status":"TO DO","labels":["[zube]: Backlog","type: reporting","client: ows","jira"],"comments":[]},{"summary":"P2P Fund: No values for WA Income stratified by loan grades P1-P5 for LC loans","externalId":"2915","repo":"aqueduct","description":"P2P Fund states that WA Annual Income does not appear for LC loans that were originated in 2021, when you stratify by the P1-P5.\r\n\r\n![image](https://user-images.githubusercontent.com/55464562/126227060-db7960cd-cd74-4f7c-b90d-581041125528.png)\r\n\r\nThe stratification is not working for that particular metric because the strat works for EOP and the WA Income metric works fine without the stratification.\r\n\r\nSee screenshots below:\r\n\r\n![image](https://user-images.githubusercontent.com/55464562/126227203-30dcdee5-c60b-420d-9dcb-a51b0c44d051.png)\r\n![image](https://user-images.githubusercontent.com/55464562/126227403-65b23455-3a35-45b8-b9c7-6c31774215be.png)\r\n","created":"2021-07-19T21:07:21.000Z","issueType":"Task","reporter":"Tito Donis","status":"TO DO","labels":["[zube]: DevStarted","data issue","client: p2pfund","jira"],"comments":[{"author":"Jay Patel","body":"It looks like this is being causes by missing gross_annual_income for these loan grades. See below for sample. I think for now I can just set gross annual income based on monthly income and it SHOULD populate the metric, but want to explore why this is not being mapped in the first place. Unfortunately its a little difficult now because of the cassandra file cleanup we did recently, so we no longer have that data loaded into cassandra.  ![Screen Shot 2021-08-13 at 12.13.00 PM.png](https://zube.io/files/peeriq/e2df59c10d52348c19f6769f86a28383-screen-shot-2021-08-13-at-12-13-00-pm.png)","created":"2021-08-13T16:32:58.000Z"}]},{"summary":"MSIM - Charge Offs % and Unscheduled Principal % Discrepancy","externalId":"2914","repo":"aqueduct","description":"MSIM is seeing zero charge off % and unscheduled principal below 1%, which are odd values compared to the pool, \"Prosper Deal May 21\". Please see the screenshot below:\r\n\r\n![image](https://user-images.githubusercontent.com/55464562/126177125-4193f961-4290-4187-bf0d-4f532918ec5f.png)\r\n","created":"2021-07-19T14:33:24.000Z","issueType":"Task","reporter":"Tito Donis","status":"TO DO","labels":["[zube]: Backlog","priority: high","client: msim","data issue","jira"],"comments":[{"author":"Tito Donis","body":"@jayp0413 Please see the loan tape attached with all of the loan IDs.\r\n[Unsaved_Loan_Tape--Prosper_Deal_May_21--2021-05-31.xlsx](https://github.com/peeriq/aqueduct/files/6843780/Unsaved_Loan_Tape--Prosper_Deal_May_21--2021-05-31.xlsx)\r\n","created":"2021-07-19T20:39:31.000Z"},{"author":"Tito Donis","body":"Original email:\r\n\r\nTitle: Re: Cashflow Model Call\r\n![image](https://user-images.githubusercontent.com/55464562/126984840-d0b29744-cb00-41bc-ab8d-9e6fc8c660a3.png)\r\n","created":"2021-07-26T11:56:31.000Z"},{"author":"Jay Patel","body":"@tdonis any other details they provided? Im looking at charge off counts historically for public prosper data  and they seem pretty consistent. Are they expecting the loans in this pool  to be charged off?\r\n<img width=\"712\" alt=\"Screen Shot 2021-08-05 at 4 17 39 PM\" src=\"https://user-images.githubusercontent.com/58573377/128415420-dfce8c1a-93ff-4fc7-85d1-8347df4da437.png\">\r\n","created":"2021-08-05T20:27:34.000Z"},{"author":"Tito Donis","body":"@jayp0413 Will revert back with them. Is the Unscheduled Principal Received matching what we have in the database?\r\n\r\nI have the list of loan IDs in this ticket.\r\n\r\n![image](https://user-images.githubusercontent.com/55464562/128518898-0930bbc4-d8a9-4d8d-aac1-e9ce20ea8784.png)\r\n","created":"2021-08-06T13:38:09.000Z"}]},{"summary":"Flexpoint Ford - Cumulative Losses Fluctuation","externalId":"2912","repo":"aqueduct","description":"Flexpoint Ford is seeing a fluctuation of cumulative losses when running their analysis on months on book, filtering by annual vintage 2015 and grade AA loans. It does not look like cumulative loan size or months on book are impacting this value significantly, but there is a fluctuation with the losses dollar amount. Could you please look into why cumulative losses are fluctuating? \r\n\r\n![image](https://user-images.githubusercontent.com/55464562/126208863-18711fc0-2dbf-4a6d-bfe9-b698f4b537d5.png)","created":"2021-07-16T19:32:19.000Z","issueType":"Task","reporter":"Tito Donis","status":"TO DO","labels":["[zube]: Backlog","bug","jira"],"comments":[]},{"summary":"OWS: Discrete Original Term Metric is showing inaccurate values","externalId":"2911","repo":"aqueduct","description":"OWS is saying that the Discrete Original Term stratification metric is showing weird values. For instance, Affirm does not have five month loans, but the platform is showing that there are. Can we make sure that the values align with what's in the servicing file for original term? FYI - this is a new metric. \r\n\r\n![image](https://user-images.githubusercontent.com/55464562/125950131-8cdd24aa-6693-4e30-ac15-eabc6c5f8b22.png)","created":"2021-07-16T12:50:58.000Z","issueType":"Task","reporter":"Tito Donis","status":"TO DO","labels":["[zube]: DevStarted","bug","jira"],"comments":[{"author":"Jay Patel","body":"@tdonis this looks to be coming from a couple loans ('819T-8OY7' and 'E0S8-8MSS') that have a value of 5 for the 'loan_term' field in the Affirm Loan Tape.\r\n\r\n<img width=\"675\" alt=\"Screen Shot 2021-07-16 at 11 19 31 AM\" src=\"https://user-images.githubusercontent.com/58573377/125970783-ed991888-5464-44a6-86e8-712a1a0cd215.png\">\r\n\r\n\r\n<img width=\"725\" alt=\"Screen Shot 2021-07-16 at 11 17 51 AM\" src=\"https://user-images.githubusercontent.com/58573377/125970613-a830ce42-db21-4d16-9632-c70461353bf9.png\">\r\n","created":"2021-07-16T15:19:54.000Z"},{"author":"Tito Donis","body":"@jayp0413 According to OWS, the irregular discrete original term values are modified loans, but the platform isn't flagging them as such. Since these loans are mods, the original term should have never changed for them. \r\n\r\nCan we change the irregular/odd values for discrete original term to their original values prior to modification?\r\n","created":"2021-07-24T15:53:18.000Z"}]},{"summary":"P2PFund: Avg. Original Principal is showing no values when stratifying by Q2 2021","externalId":"2910","repo":"aqueduct","description":"The Avg Orig Principal metric is showing no values when stratifying for Q2 2021 for P2PFund. Since the BOP balance was higher for Q1 2021, I would also expect for the average to be higher. Please see screenshot below. I have added the BOP metric as well for reference:\r\n\r\n![image](https://user-images.githubusercontent.com/55464562/125887397-97e4b724-0973-44cb-b086-8c580a4c2a08.png)\r\n","created":"2021-07-16T03:31:30.000Z","issueType":"Task","reporter":"Tito Donis","status":"TO DO","labels":["[zube]: Backlog","bug","client: p2pfund","jira"],"comments":[]},{"summary":"OWS - 1-to-2 Missed Payments % is too high in the OWS: All pool","externalId":"2907","repo":"aqueduct","description":"Using the OWS: All pool, the 1-to-2 Missed Payments % is quite high showing values over 85% for the pool. This might be driven by low amounts in the Prior Status 1 Missed Payment ($) metric, which is used to calculate 1-to-2 Missed Payments %. \r\n\r\nTime Series:\r\n![image](https://user-images.githubusercontent.com/55464562/125546874-5b46774a-b3bf-462b-af4c-540254b5a312.png)\r\n\r\nRoll Rates is showing this to be ~11.34% \r\n![image](https://user-images.githubusercontent.com/55464562/125546958-5e8a07b9-d9fb-4dfc-b13a-0a02396878a9.png)\r\n\r\nAttached is a spreadsheet where I calculated the 1-to-2 Missed Payments % using values from the platform. See columns highlighted in yellow: \r\n[OWS ALL_Missed Payments %.xlsx](https://github.com/peeriq/aqueduct/files/6812904/OWS.ALL_Missed.Payments.xlsx)\r\n","created":"2021-07-14T01:41:43.000Z","issueType":"Task","reporter":"Tito Donis","status":"TO DO","labels":["[zube]: DevStarted","data issue","client: ows","jira"],"comments":[]},{"summary":"OWS Data Feed Encryption","externalId":"2905","repo":"aqueduct","description":"http://taskmanager.peeriq.com:8000/v1/admin/data_feed/transmissionprocess/2287/change/\r\n![Screen Shot 2021-07-13 at 9.39.17 AM.png](https://zube.io/files/peeriq/b554b3862a9cf451e4f719fa43002732-screen-shot-2021-07-13-at-9-39-17-am.png)","created":"2021-07-13T13:43:31.000Z","issueType":"Task","reporter":"Jay Patel","status":"TO DO","labels":["[zube]: DevStarted","client: ows","jira"],"comments":[]},{"summary":"Compliance Items","externalId":"2898","repo":"aqueduct","description":"Few items to send over to @alamdarmcs1 regarding compliance regulations-\r\n1. List of active originators + clients and  how we are receiving data(SFTP/API/Email)\r\n2. Confirm with gaurav that sftp is set up separate per client\r\n3. Check for any PII data that we are receiving from any originator and flag it in a list\r\n4. Potentially add completion alert for ETL jobs","created":"2021-07-07T20:57:14.000Z","issueType":"Task","reporter":"Jay Patel","status":"TO DO","labels":["[zube]: DevStarted","ISO/SOC Compliance","jira"],"comments":[{"author":"Jay Patel","body":"@alamdarmcs1 for item 1 [File](https://zube.io/files/peeriq/600104640e1bba71f31f71ca5bd3cafb-active_client_originator_pairs.xlsx)","created":"2021-07-12T20:52:44.000Z"},{"author":"Jay Patel","body":"item 3- PII Data [PII_Data_all_originators.xlsx](https://zube.io/files/peeriq/de716d55800c948621a376eb2b843c58-pii_data_all_originators.xlsx)","created":"2021-07-19T18:34:03.000Z"}]},{"summary":"LendingPoint Files not pulling","externalId":"2897","repo":"aqueduct","description":"We are getting authentication issues when trying to pull LendingPoint files, think we just need to rotate the key.\r\n![Screen Shot 2021-07-07 at 2.50.10 PM.png](https://zube.io/files/peeriq/55b61e2d3c0c28c9548bf9d49b27f6ae-screen-shot-2021-07-07-at-2-50-10-pm.png)","created":"2021-07-07T18:55:26.000Z","issueType":"Task","reporter":"Jay Patel","status":"TO DO","labels":["[zube]: DevStarted","client: lendingpoint","jira"],"comments":[]},{"summary":"OWS - Missing purchases from \"OWS: All\" pool for 6/15 and 6/16 ","externalId":"2880","repo":"aqueduct","description":"OWS said that they made purchases on 6/15 and 6/16, but the last purchase the platform is showing is on 6/14 for the pool. Please see the screenshot below:\r\n\r\n![image](https://user-images.githubusercontent.com/55464562/123312928-f016c700-d4f6-11eb-9d28-505d11f9b410.png)\r\n","created":"2021-06-24T18:18:28.000Z","issueType":"Task","reporter":"Tito Donis","status":"TO DO","labels":["[zube]: Backlog","priority: high","bug","client: ows","jira"],"comments":[{"author":"Tito Donis","body":"@jayp0413 Thanks for refreshing the data yesterday. Can we investigate what might have happened? OWS is wondering why the purchases were missing. ","created":"2021-06-25T21:08:31.000Z"},{"author":"Tito Donis","body":"@jayp0413 We're missing purchases again for OWS. This time it's for the OWS: All pool. Specifically for 8/12 and 8/13 according to NIck. Any reason why this is happening? Do we just need to refresh?\r\n\r\n![image](https://user-images.githubusercontent.com/55464562/129937473-64c3ae5c-547d-4536-b479-e0a32fe0a001.png)\r\n","created":"2021-08-18T16:39:08.000Z"},{"author":"Jay Patel","body":"No clue why purchases arent being picked up randomly for those days- but it seems like rerunning the data fixed this just like last time. ","created":"2021-08-23T03:41:08.000Z"},{"author":"Tito Donis","body":"@jayp0413 OWS is saying that purchases are missing again. Is there a way that we can check this after an upload to make sure it matches the purchases in the file or a way that we can automatically capture these? He's glad that we have a fix, which is to refresh the data, but he really wants us to figure out the root cause of the issue:\r\n![image](https://user-images.githubusercontent.com/55464562/130807397-0d8648f2-7597-42e1-bca1-b55cb8f18935.png)\r\n\r\n","created":"2021-08-25T14:17:15.000Z"},{"author":"Jay Patel","body":"Hmm I have an initial guess as to why this loading gets messed up, so the repurchase file comes in a day before the loan tape( ex 8/23 repurchase file actually comes into our sftp at 8/23 and the other files for that day come in 8/24) ex here:\r\n<img width=\"858\" alt=\"Screen Shot 2021-08-25 at 11 54 36 AM\" src=\"https://user-images.githubusercontent.com/58573377/130825465-3c9bd422-21ff-4b9d-b357-d2bd6ab9ab46.png\">\r\nThe 8/23 job is being kicked off by airflow on 8/23, which is before the other files are there(this should not be happening).\r\n<img width=\"1417\" alt=\"Screen Shot 2021-08-25 at 12 23 16 PM\" src=\"https://user-images.githubusercontent.com/58573377/130828276-8bb7b475-d69c-46a6-8cf6-fefd2944f026.png\">\r\n Therefore the principal purchased, along with other items are not accurately updated on the platform. see below: \r\n<img width=\"727\" alt=\"Screen Shot 2021-08-25 at 12 22 26 PM\" src=\"https://user-images.githubusercontent.com/58573377/130828109-c562f47e-f365-44f0-92eb-4f45f2a9190b.png\">\r\n\r\nThe solution here is to make sure Airflow is kicking off the jobs only when all 3 files are available for the specific dates that we do receive a repurchase file.\r\n \r\n","created":"2021-08-25T16:41:22.000Z"},{"author":"Tito Donis","body":"@jayp0413 Thanks for the detailed response. The proposed fix would make sense. \r\n\r\nA few questions:\r\n\r\n1. Can we refresh the data for missing purchases?\r\n2. When can we implement the change for airflow to kick off when other files are available?\r\n3. Will this change have any delays in the data upload?","created":"2021-08-25T16:51:43.000Z"},{"author":"Jay Patel","body":"@tdonis \r\n1. yep loading it now, will let you know when its updated\r\n2. I'll look into it today, I will also sync up with Rich on this \r\n3. There wont be any delays after this change, data will still be uploaded on the expected date.","created":"2021-08-25T16:55:14.000Z"}]},{"summary":"Update PenFed pools","externalId":"2856","repo":"aqueduct","description":"We have 4 pools for PenFed:\r\n\r\nPenFed - Upgrade\r\nPenFed: All - Upgrade\r\nPenFed: All - Prosper\r\nPenFed: All\r\n\r\nPenFed: All is sum of PenFed: All - Upgrade and PenFed: All - Prosper and those 3 seem to be correct\r\n@barryPIQ can you check what PenFed - Upgrade pool is? it seems to be an incorrect pool\r\n\r\nAlso @jayp0413 can you update the monthly tables for those 3 pools? daily is updated but monthly is not","created":"2021-06-09T15:21:39.000Z","issueType":"Task","status":"TO DO","labels":["[zube]: Backlog","priority: high","jira"],"comments":[{"author":"Jay Patel","body":"monthly data is updated for these now @sahaimed ","created":"2021-06-09T18:45:59.000Z"},{"author":"Barry Ickow","body":"looks like this one Done @jayp0413 ?","created":"2021-07-08T17:00:34.000Z"},{"author":"Barry Ickow","body":"oh there's a question on there for me, will try to do this sprint (\"PenFed: All is sum of PenFed: All - Upgrade and PenFed: All - Prosper and those 3 seem to be correct\r\n@barryPIQ can you check what PenFed - Upgrade pool is? it seems to be an incorrect pool\")","created":"2021-07-08T17:01:54.000Z"},{"author":"Jay Patel","body":"yep, data part is done, just close it out after you check that @barryPIQ ","created":"2021-07-08T17:02:27.000Z"}]},{"summary":"MSIM datafeed: Upstart fields missing","externalId":"2854","repo":"aqueduct","description":null,"created":"2021-06-08T21:00:05.000Z","issueType":"Task","status":"TO DO","labels":["[zube]: Backlog","jira"],"comments":[]},{"summary":"Clear Cache for Data Models not working","externalId":"2848","repo":"aqueduct","description":"sample output for a build attempt- http://jenkins.peeriq.com/view/Cache%20Clears/job/clear-PROD-cache-Client-DataModel-Table-Paramater/1055/console\r\n\r\nFor now we can use the clear cache all.","created":"2021-05-28T16:12:13.000Z","issueType":"Task","reporter":"Jay Patel","status":"TO DO","labels":["[zube]: Backlog","jira"],"comments":[]},{"summary":"OWS Cash Settlement Report Logic","externalId":"2847","repo":"aqueduct","description":"See BRD from product team. On a high level, we are capturing the latest status of each transaction across all servicing files, replace file date with first file date it appears from and remove all dupes based on event_id to sum the expected sweep amount and sweep amount. \r\n\r\n#### Functional steps:\r\n1. Combine all servicing files in the s3 bucket into one dataframe\r\n2. Sum the expected sweep amount and sweep amount for each event_id based on combination of 'file_date','sweep_date','posting_date','event_id','charge_ari'\r\n3. Drop duplicates based on 'posting_date','event_id' and keep the last entry (because we want the latest status) \r\n4. Update the file date column with first file date entry of the event id\r\n5. Save into csv.\r\n\r\n#### Expected output (not the pivot tab) as of servicing files 5/25: \r\n[cash_settlement_report.xlsx](https://zube.io/files/peeriq/349abc8cee04ccd3e144180809a64477-cash_settlement_report.xlsx)\r\n\r\n#### PeerIQ output:\r\nAble to match the output except for the minor discrepancies (2 records) raised to OWS on 5/27.\r\n![image.png](https://zube.io/files/peeriq/be872b9bdc6ceb7eee9b74a53d6fb6a8-image.png)[ows_affirm_cash_settlement_report_2021-05-25.xlsx](https://zube.io/files/peeriq/b4a306e6cd9ff835dca8065bdad328e7-ows_affirm_cash_settlement_report_2021-05-25.xlsx)\r\n\r\nThe steps to recreate the output is in a jupyter notebook: Google Drive \r\nhttps://drive.google.com/drive/folders/12VoLgLkCjuV3iTUHUtq92Sns0LV7oOX9\r\n\r\n\r\n\r\n### Next Steps:\r\n- [x] Logic to create the report based on PIQ's output should be correct. Nick's initial output was missing a few edge cases which he acknowledged on the email. \r\n- [ ] Assess the implementation effort to create this report and the process to support daily going forward. There are many ways to run this report with the script already created. \r\nEg. \r\noption 1: Celery or Airflow to run the shell script to produce the excel report and sends the email to Nick. No man dependency. Requires 1 week automation effort from data team. (Engineering effort - its worth it if we are building out 3 reports and the service can be extendable to other reporting needs)\r\noption 2: Somebody runs the script and sends the report to Nick. Dependency on data team to be available.\r\noption 3:","created":"2021-05-27T22:25:04.000Z","issueType":"Task","status":"TO DO","labels":["[zube]: DevStarted","client: ows","jira"],"comments":[]},{"summary":"Test Load 500K data for OWS CFM","externalId":"2843","repo":"aqueduct","description":"OWS will likely have 500K loan data in 12 weeks time (early-mid August) and we need to prepare to scale up our ETL job to process this data within 75 min (max 90 min).\r\n\r\nCurrently with 180K loans - processing time is average 60min. \r\n\r\nThe bottleneck is the `Copy Table From New ETL 1.0` that will increase over time because it takes a long time to copy data from .csv into table in Redshift.","created":"2021-05-24T16:00:42.000Z","issueType":"Task","status":"TO DO","labels":["[zube]: Backlog","client: ows","jira"],"comments":[{"body":"1. Randomized loan ids for servicing and loan tape file based on production file in folder 20210520.\r\n700k+ loans -- dummy files are in s3 `/peeriq-etl/etl/Input/Test/Affirm/Client/20210520`, ready for testing. \r\n![image.png](https://zube.io/files/peeriq/2a722e2441dfd4f5ce06872b4cf38b90-image.png)\r\n\r\n2. Next steps: Deploy in prod (because resource vcpu in prod is more performant) and load OWS CFM data as client = `Test`. Monitor run time.\r\nhttp://taskmanager.peeriq.com:8000/v1/admin/task_manager/workflowinstance/28581/change/","created":"2021-05-24T17:57:53.000Z"},{"body":"Test run for 700k+ loans takes 128 minutes to complete. \r\n\r\nThe biggest bottle neck is the task `Copy Table From New ETL 1.0` that took 50min to run. \r\n\r\nEach task duration can be found here: http://taskmanager.peeriq.com:8000/v1/admin/task_manager/workflowinstance/28581/change/\r\n\r\n@zvika-peeriq @jayp0413 @petercao-peeriq @sahaimed \r\n\r\nThe generic Copy Table Task without deduping which @barryPIQ  and @Rich-Hart are currently working on to resolve the CRB data load issue (https://zube.io/peeriq/p1r1/c/24429) will help to speed up this process, but still need to evaluate how much time it would save. ","created":"2021-05-25T17:19:32.000Z"}]},{"summary":"Ability to repoint ETL test env to read from Cassandra DB production database","externalId":"2804","repo":"aqueduct","description":"We would like the ability to run ETL jobs in test env (must have vpn to access http://test.taskmanager.peeriq.com/) with production data read from Cassandra Prod Database. \r\n\r\nCurrently if you run an ETL job in test env, it picks up the data from Cassandra Test which is not ideal because test env is more than likely to not have prod files...","created":"2021-04-30T18:07:49.000Z","issueType":"Task","status":"TO DO","labels":["[zube]: DevStarted","type: improvement","type: feature","jira"],"comments":[{"author":"Barry Ickow","body":"more specifically, a Test workflow should be able read, and only read, from Prod (prod cassandra, prod S3, prod RedShift / Vertica, etc.) (and read and write from/to Test of course as well) - if a Test workflow tries to write to anything in Prod, the process should error","created":"2021-07-01T21:00:24.000Z"}]},{"summary":"MDS File/Testing Updates","externalId":"2778","repo":"aqueduct","description":"Creating a ticket to cover any items in the MDS that we need to look into. We can just keep a running log of any MDS csv related items we want to update/look into on this ticket.\n\n@barryPIQ First one is for creating a unit test to flag and duplicated IDs used in the `transform_file.csv`. We want to validate that all ids are unique so circleci can catch any conflicts on this prior to merging.\n\n![Screen Shot 2021-04-20 at 10.51.26 AM.png](https://zube.io/files/peeriq/f71198aff2666def59a7a91bf9461356-screen-shot-2021-04-20-at-10-51-26-am.png)","created":"2021-04-20T16:20:32.000Z","issueType":"Task","reporter":"Jay Patel","status":"TO DO","labels":["[zube]: Backlog","service: metadata_service","jira"],"comments":[]},{"summary":"OWS Affirm - Modification indicator (Validation)","externalId":"2769","repo":"aqueduct","description":"Check if ever modified is set up correctly. \r\n\r\n![image.png](https://zube.io/files/peeriq/7898f68e69af9a961e3de1429113d0d0-image.png)","created":"2021-04-08T13:08:38.000Z","issueType":"Task","status":"TO DO","labels":["[zube]: Backlog","client: ows","jira"],"comments":[]},{"summary":"CRB Pool Requests","externalId":"2755","repo":"aqueduct","description":"@tdonis I believe this can be achieved on the pool level, please take a look when you can.\r\n![Screen Shot 2021-03-31 at 12.44.11 PM.png](https://zube.io/files/peeriq/036816fd14861f8f928e6c7514da319d-screen-shot-2021-03-31-at-12-44-11-pm.png)","created":"2021-03-31T16:53:07.000Z","issueType":"Task","reporter":"Jay Patel","status":"TO DO","labels":["[zube]: DevStarted","originator: rocket","marlette","client: Cross River","jira"],"comments":[{"author":"Jay Patel","body":"https://github.com/peeriq/iris/pull/6933","created":"2021-03-31T16:53:14.000Z"},{"author":"Jay Patel","body":"Short term solution for item 1- we manually removed the entries from the crossriver table where the current owner is not 'crbRetained'.(this is complete)\r\n\r\nLong term solution is to load Rocket data into the public table instead, and copy the crbRetained loans into the crossriver table in the copy table task. We want to get this in prior to next months Rocket Upload.","created":"2021-04-07T15:18:07.000Z"},{"body":"@jayp0413 reminder to close out #1 and let CRB know","created":"2021-04-09T18:04:42.000Z"},{"author":"Jay Patel","body":"https://github.com/peeriq/iris/pull/7030","created":"2021-07-07T17:23:19.000Z"}]},{"summary":"Improve Platform Analytics","externalId":"2745","repo":"aqueduct","description":"Look into providing more statistically driven analytics on the platform. Correlation between fields, predictive analytics, etc. Probably good to brainstorm with client delivery team to figure out what would be useful for clients to be able to see.","created":"2021-03-26T15:51:08.000Z","issueType":"Task","reporter":"Jay Patel","status":"TO DO","labels":["[zube]: Backlog","jira"],"comments":[]},{"summary":"SoFi New Forbearance Field","externalId":"2743","repo":"aqueduct","description":"Incoming new field for sofi files- will have total # of months a loan has been in forbearance.\r\n![Screen Shot 2021-03-25 at 4.16.52 PM.png](https://zube.io/files/peeriq/ce0ad1270bb91112ff718823a88bcc68-screen-shot-2021-03-25-at-4-16-52-pm.png)\r\n\r\n[PL Standard Reporting 20210325.xlsx](https://zube.io/files/peeriq/d3679542fc9facbd4b3a79efff66fd6f-pl-standard-reporting-20210325.xlsx)\r\n\r\n[PL_servicing_report_TEST_2010325.xlsx](https://zube.io/files/peeriq/79ebc07107c6bfdfc56d8fd26ed1dcc0-pl_servicing_report_test_2010325.xlsx)\r\n\r\n[PL_servicing_report_TEST_2010325.txt](https://zube.io/files/peeriq/4f463a37356c340a0f003f848175ccf7-pl_servicing_report_test_2010325.txt)","created":"2021-03-25T20:21:39.000Z","issueType":"Task","reporter":"Jay Patel","status":"TO DO","labels":["[zube]: Backlog","originator: sofi","jira"],"comments":[{"author":"Jay Patel","body":"The change is not live yet as of 4/15 files.\r\n\r\nNot sure if we need to map this field at all- maybe if loans are able to re enter forbearance so there are 2 separate periods of forbearance we could incorporate this into the logic. Right now we are deriving forbearance age from the forbearance start/end dates and file date.","created":"2021-04-16T13:50:31.000Z"}]},{"summary":"Update Rocket ETL destination","externalId":"2740","repo":"aqueduct","description":"-Need to change Rocket data to load into public table instead of CR table.\r\n-CR table![Screen Shot 2021-03-25 at 11.49.01 AM.png](https://zube.io/files/peeriq/697508f733b74b3dd3edbcff584361f2-screen-shot-2021-03-25-at-11-49-01-am.png) should contain subset of that data based on current owner","created":"2021-03-25T15:50:44.000Z","issueType":"Task","reporter":"Jay Patel","status":"TO DO","labels":["[zube]: Backlog","originator: rocket","client: Cross River","jira"],"comments":[{"author":"Jay Patel","body":"Tito was able to create the 2 pools requested for this- low priority to update the ETL right away","created":"2021-03-25T18:34:32.000Z"}]},{"summary":"Tableau Dashboard for Error Workflows","externalId":"2713","repo":"aqueduct","description":"Previously generated using Tableau Public.\r\n\r\nMigrate/Recreate using Tableau Desktop","created":"2021-03-16T13:33:14.000Z","issueType":"Task","status":"TO DO","labels":["[zube]: Backlog","type: reporting","jira"],"comments":[]},{"summary":"Time Based Indices for ElasticSearch for easy, automated pruning of old logs","externalId":"2697","repo":"aqueduct","description":"In order to prevent https://zube.io/peeriq/p1r1/c/24421 , we should most likely be using time based indices when writing logs to ES and use Index Lifecycle Management / Policies to delete each index as it gets \"cold\" (haven't been used in x amount of time) - deleting individual logs is not recommended as it is a little more cumbersome in ES and computationally intensive for ES, and continuing to add more hardware is probably not cost effective\r\n\r\nAlso, using the Rollover API is probably more difficult for our case (would probably have to be size based and even that might not work, ie: what if we rollover a large index for deletion while it's being queried? )","created":"2021-03-05T21:07:32.000Z","issueType":"Task","reporter":"Barry Ickow","status":"TO DO","labels":["[zube]: Backlog","Perf","jira"],"comments":[]},{"summary":"MDS endpoint failing on Batch","externalId":"2623","repo":"aqueduct","description":"https://console.aws.amazon.com/cloudwatch/home?region=us-east-1#logsV2:log-groups/log-group/$252Faws$252Fbatch$252Fjob/log-events/data-transformer-prod$252Fdefault$252F9be82e930b464c3e8b1adeb8b1257b01","created":"2021-01-28T17:10:58.000Z","issueType":"Task","status":"TO DO","labels":["[zube]: Backlog","jira"],"comments":[{"body":"This is failing for marlette data with 1 years duration period\r\n\r\nsample error from mds pod\r\n\r\n![image.png](https://zube.io/files/peeriq/f2d78f25f88410a3dbd2131abeae3d20-image.png)\r\n\r\n\r\nsample error from cloudwatch log\r\n![image.png](https://zube.io/files/peeriq/f717957035e5f10a892c2080ceed14a9-image.png)","created":"2021-01-29T14:22:04.000Z"},{"body":"now it keeps failing for marlette even if i redeploy mds. \r\n\r\nNot sure `...The work of process 50 is done. Seeya!` if the requests has something to do with uWSGI? or django redis decoding error?\r\n![image.png](https://zube.io/files/peeriq/abfd857d89d9cc862f1d563e8073e957-image.png)","created":"2021-01-30T14:37:31.000Z"}]},{"summary":"Set up Upstart metadata using Reporting API - Q1 2021","externalId":"2507","repo":"aqueduct","description":"Need to set up new data transformation process using data in Reporting API (ie. field mapping, transforms) \r\n\r\nThis is somewhat dependent on Upstart making all attributes available there in Q1 2021. https://github.com/peeriq/aqueduct/issues/2403\r\n\r\nCurrently we have information in the loans and repayments file. Can probably start doing the mapping, and when new endpoint and other details available, we can focus on data transformation. ","created":"2020-12-04T18:20:30.000Z","issueType":"Task","status":"TO DO","labels":["[zube]: DevStarted","originator: upstart","jira"],"comments":[{"body":"Upstart:\r\n\r\nRecoveries\r\nYou are correct that the Recovery Paid field only reflects payments made by the borrower on or after the loan's charge off date. This is based on the payments that have been remitted. This will be 'recovery-remitted' once you're utilizing the Reporting API.\r\n\r\nTotal Payment Amount = \r\nFor the Reporting API, this includes 'principal-paid', 'interest-paid' and 'borrower-fees-remitted'. We define the 'Amount' field in the /repayments endpoint and borrower payments file as the total amount sent for this payment by the borrower. This payment is allocated to interest, fees, and principal once the status is completed. ","created":"2020-12-21T14:12:56.000Z"},{"author":"Jay Patel","body":"Full API documentation not available at the moment, only whitelisted IPs are allowed to access it. We only have our EC2 machines added so waiting on a solution from Upstart","created":"2021-06-17T18:48:28.000Z"}]},{"summary":"Confirm can write to Vertica from AWS Batch (for removing RedShift)","externalId":"2129","repo":"aqueduct","description":"Probably can just add one dummy line at the end of postprocess_files in file_transformer.py that writes one dummy row to the errorchecks_public table in vertica-test (data-transformer runs on AWS batch, which will have to be able to write to Vertica to get rid of services currently relying on RedShift)","created":"2020-07-07T18:41:52.000Z","issueType":"Task","reporter":"Barry Ickow","status":"TO DO","labels":["[zube]: Backlog","jira"],"comments":[{"author":"Barry Ickow","body":"Also, before doing any AWS Batch in place of RedShift work - the cost between running Batch vs just running in Vertica (since the base tables most likely have to be stored there whether Batch or no Batch) has to be compared / confirmed.","created":"2020-07-10T13:57:29.000Z"},{"author":"Barry Ickow","body":"Actually, the fact that `Calc Rules Async` exists (which specifically added the step of moving the time intensive calc rules from RedShift to AWS Batch) probably means AWS Batch is the way to go","created":"2020-07-10T14:01:55.000Z"},{"author":"Barry Ickow","body":"However, a MUST to confirm before doing any of this work is also confirming that EMR rollups can run from Vertica as opposed to RedShift","created":"2020-07-10T14:15:06.000Z"}]},{"summary":"Investigate how ProcessTULoanSparkJob work","externalId":"2006","repo":"aqueduct","description":"This process is used to process data that is stored in S3 HDFS bucket","created":"2020-05-28T01:46:41.000Z","issueType":"Task","status":"TO DO","labels":["[zube]: DevStarted","jira"],"comments":[]},{"summary":"ETL Architecture Diagram Overview","externalId":"1928","repo":"aqueduct","description":"should include:\r\n- Celery\r\n- TaskManager\r\n- MDS\r\n- CSV2Cassandra\r\n- Data Transformer\r\n- EMR Rollups\r\n- Status Service\r\n- S3\r\n- Queues\r\n- Databases\r\n- anything else materially involved","created":"2020-05-06T21:35:48.000Z","issueType":"Task","reporter":"Barry Ickow","status":"TO DO","labels":["[zube]: Backlog","jira"],"comments":[{"body":"Adding here:\r\n- Airflow\r\n\r\n- Deming (the last workflow instance pulls deming API (see iris repo `deming_task.py`) \r\n\r\n- Superset UI (i think this was built by Lulu? Andrei wasn't familiar with it as far as I rmb...) -- but it is connected to DBs and is a dashboard that shows all the ETL jobs loaded in vertica as of latest date. http://superset.test.peeriq.com/superset/welcome\r\n![image](https://user-images.githubusercontent.com/48953655/81326737-3e090b00-9068-11ea-98ff-686985950043.png)  ping me for your login credentials. \r\n","created":"2020-05-07T17:42:00.000Z"},{"author":"Barry Ickow","body":"and also, do we still have descriptive-statistics, environment-comparison, or exception-checker services? (the names should be at least close to that if they exist)\r\n\r\nthe purpose of the diagram is to see the moving parts and their connections, and then to also see in what areas we can try to save more money without losing performance","created":"2020-05-13T03:15:27.000Z"},{"body":"@barryPIQ i think we do... and just skimming through the readme, it looks like this is similar to Deming but checking data through the workflow before data is copied into vertica? I think these are very useful if we actually know how to utilize it and run? .(at least for descriptive stats and exception checker).. not sure about environment comparison. \r\n\r\n","created":"2020-05-13T13:27:37.000Z"}]},{"summary":"Adjust upload error message","externalId":"1896","repo":"aqueduct","description":"When we try to upload a file that has extra columns, the upload fails but the message suggests that the upload succeeded but without the extra columns. \r\n\r\nI think it should either:\r\n\r\n  1. Upload the file without the extra columns\r\n  2. Provide a message saying that it didn't upload\r\n\r\n![image](https://user-images.githubusercontent.com/2336431/80608651-14ffcf00-8a05-11ea-9d93-0640b17ca192.png)\r\n","created":"2020-04-29T14:35:14.000Z","issueType":"Task","status":"TO DO","labels":["[zube]: Backlog","jira"],"comments":[]},{"summary":"Test Run Automatic Data Feed","externalId":"1851","repo":"aqueduct","description":"https://github.com/peeriq/iris/pull/6608/files\r\n\r\nInclude this yml in a workflow and test","created":"2020-04-17T15:05:19.000Z","issueType":"Task","status":"TO DO","labels":["[zube]: Backlog","type: data feed","jira"],"comments":[{"body":"https://github.com/peeriq/iris/pull/6616","created":"2020-04-17T21:35:26.000Z"},{"body":"@barryPIQ \r\nI added your script into my yml file (https://github.com/peeriq/iris/pull/6616/files)\r\nRedeployed etl test --> tag test-data-feed\r\nBut I couldn't run it because it doesn't recognized it. Just looking at my PR -- do you know what is wrong with it? \r\n\r\n![image.png](https://zube.io/files/peeriq/52b53d076c1880261e50114765febbd5-image.png)","created":"2020-05-18T20:31:00.000Z"},{"body":"@barryPIQ looks like a date formatting issue. Is this something you can look into? \r\n\r\n![image.png](https://zube.io/files/peeriq/638e954865a823d5cc2cffb7f7dc1fb2-image.png)","created":"2020-05-19T17:03:16.000Z"},{"author":"Barry Ickow","body":"hm, ya will do @jojoms711 ","created":"2020-05-19T19:06:21.000Z"},{"body":"Data feed successfully sent to sftp test location!\r\n![image.png](https://zube.io/files/peeriq/414445b2a484659b35f9ae38673836e8-image.png)","created":"2020-05-20T23:08:07.000Z"},{"body":"@barryPIQ this is great work!!! Exactly what i needed.\r\n\r\n1) Ran without auto run enabled, workflow runs to completion but no feed generated. \r\n\r\n2) Enabled the auto run, workflow runs to completion. And it would show up on the data feed transmission console as sent which is helpful for anybody to check when the last feed was sent. \r\n![image.png](https://zube.io/files/peeriq/2810e8891827891bfc13d35716fb8cac-image.png)\r\n\r\n![image.png](https://zube.io/files/peeriq/94d04cc56816b5c247428f542020a70d-image.png)\r\n\r\n![image.png](https://zube.io/files/peeriq/36ed90260c3862ff11975d8b21462c7a-image.png)","created":"2020-05-27T01:16:39.000Z"},{"body":"@barryPIQ one more Q --- i noticed the DB env was from `prod`...is that a hardcoded value? Data was copied to vertica-staging and i see it in `/peeriq-emr-test/tables/rollup_tables/processedloanborrower_morganstanley_sofi_daily/d58ff837-bc47-4748-ac7f-418100223398`... \r\n\r\n![image.png](https://zube.io/files/peeriq/94d04cc56816b5c247428f542020a70d-image.png)\r\n\r\nhttp://test.taskmanager.peeriq.com:8000/v1/admin/task_manager/workflowinstance/3392/change/#/tab/inline_0/","created":"2020-05-27T01:21:25.000Z"},{"author":"Barry Ickow","body":"glad it works!\r\n\r\nand ya - that's correct that is hardcoded actually - let me know if you want it based on whether you're running in test or prod - pros and cons to either way probably","created":"2020-05-27T13:11:56.000Z"},{"body":"@barryPIQ hmm what is the con? I rather it not show up in the console then if that's easier. ","created":"2020-05-27T13:15:28.000Z"},{"author":"Barry Ickow","body":"you're probably right actually - let me change that one - should be easy to make it based on the env it's running in","created":"2020-05-27T13:17:30.000Z"},{"author":"Barry Ickow","body":"hey @amitoj9 \r\n\r\nknow what the issue is here?\r\n\r\n```\r\nCOPY MorganStanley_d58ff837_bc47_4748_ac7f_418100223398\r\n                 (period_end_date,originator_loan_id,originator_note_id,originator_borrower_id,loan_month,period_start_date,issue_date,original_term,interest_rate,interest_rate_type,interest_rate_reset_period,benchmark,benchmark_value,spread_above_benchmark,apr,original_loan_principal,months_on_book,months_on_balance,outstanding_principal_bop,outstanding_principal_eop,originator_loan_status,loan_status,charge_off_indicator,charge_off_amount,charge_off_date,principal_received,principal_credits,interest_received,interest_credits,fees_received,fee_credits,total_payment_amount,total_payment_due,contractual_loan_payment,unscheduled_principal,interest_capitalization_amount,interest_capitalization_frequency,active_bankcard_accounts,active_revolving_accounts,average_current_balance,bankcard_open_to_buy,bankcard_utilization_percentage,bankruptcy_indicator,bankruptcy_status_description,borrower_custom_field_1,borrower_custom_field_2,borrower_custom_field_3,borrower_custom_field_4,borrower_custom_field_5,charge_offs_in_past_12_months,collections_last_12_months_ex_medical,current_dq_accounts,dq_accounts_past_2_years,dti_ex_mortgage,dti_inclusive,dti_monthly_payment_ex_mortgage,earliest_credit_line,employer,employment_status,employment_title,employment_verification_indicator,gross_annual_income,home_ownership_indicator,originator_home_ownership_indicator,income_verification_indicator,originator_income_verification_indicator,inquiries_in_the_past_6_months,last_credit_pull_date,latest_bankruptcy_discharge_date,latest_bankruptcy_dismissal_date,latest_bankruptcy_filing_date,length_of_employment_verification_indicator,monthly_gross_income,months_since_90_dpd,months_since_last_derogatory_event,months_since_last_dq,months_since_last_public_record,months_since_most_recent_account_opened,months_since_most_recent_bank_card_opened,months_since_most_recent_bankcard_deliquincy,months_since_most_recent_inquiry,months_since_most_recent_revolving_account_dq,months_since_most_recent_revolving_account_opened,months_since_oldest_installment_account_opened,months_since_oldest_revolving_account_opened,new_accounts_opened_in_the_past_12_months,new_accounts_opened_in_the_past_24_months,ninety_dpd_accounts_current,number_of_accounts_90_dpd_in_the_past_2_years,number_of_accounts_sent_to_collections_in_past_12_months,number_of_bankruptcies,number_of_installment_accounts,number_of_mortgage_accounts,number_of_personal_guaranties,number_of_public_records,number_of_revolving_accounts,number_of_revolving_accounts_with_balance_gt0,number_of_satisfactory_accounts,number_of_satisfactory_bankcard_accounts,number_of_tax_leins,one_hundred_twenty_dpd_accounts_current,one_hundred_twenty_dpd_accounts_ever,open_revolving_accounts,origination_credit_score,origination_credit_score_inquiry_type,origination_credit_score_range_high,origination_credit_score_range_low,origination_credit_score_type,originator_employment_length,past_due_amount,payment_to_income,peer_iq_borrower_id,employment_length,percentage_of_accounts_that_have_never_been_dq,percentage_of_bankcards_with_gt75pct_utilization,revolving_accounts_opened_in_the_last_24_months,revolving_utilization,sixty_dpd_accounts_current,thirty_dpd_accounts_current,thirty_dpd_accounts_in_past_2_years,total_accounts,total_balance,total_balance_ex_mortage,total_bankcard_accounts,total_bankcard_limit,total_collections_ever_owed,total_credit_limit,total_credit_lines,total_installment_balance,total_installment_credit_limit,total_open_credit_lines,total_personal_bankruptcies,total_real_estate_balance,total_revolving_balance,total_revolving_limit,last_updated_credit_score,last_updated_credit_score_inquiry_type,last_updated_credit_score_range_high,last_updated_credit_score_range_low,zip,last_updated_credit_score_type,accrued_interest,ach_indicator,amortization_profile,application_date,benefit_payment_hurdle,benefit_percentage,benefit_program_type,benefit_remaining_payments,benefit_status,business_day_convention,charge_off_reason,charge_off_sale_date,charge_off_sale_fees,charge_off_sale_proceeds,charge_off_sale_status,city,contractual_maturity_date,contractual_note_payment,country,current_owner,current_term,currently_paying,cycle_count,cycle_day,cycle_denomination,cycle_length,data_source,daycount_convention,days_past_due,days_since_last_payment,disbursal_date,dti_monthly_payment_mortgage_inclusive,ever_modified_indicator,expected_default_rate,expiration_date,fee_payment_due,first_contractual_payment_date,interest_payment_due,interest_purchased,client,investor_yield,joint_indicator,last_payment_amount,last_payment_date,listing_date,loan_custom_field_1,loan_custom_field_2,loan_custom_field_3,loan_custom_field_4,loan_custom_field_5,loan_day,loan_description,loan_modification_indicator,loan_past_due_amount,loan_purpose,loan_servicer,metropolitan_statistical_area,misc_credits,misc_investor_fees,model_irr,modification_end_date,modification_start_date,modification_type,months_since_last_modification,next_payment_date,next_payment_due,number_of_missed_payments,original_note_principal,original_purchaser,origination_fee,originator_loan_grade,originator_loan_score,originator_loan_sub_grade,originator_product_type,originator_sub_product_type,outstanding_fee_balance,currency,account_name,payment_frequency,payment_plan_indicator,payoff_amount,peeriq_loan_grade,peeriq_loan_sub_grade,percent_ownership,policy_code,post_charge_off_recoveries,post_charge_off_recovery_fee,post_code,premium_purchased,principal_payment_due,principal_purchased,prior_days_past_due,product_type,promo_end_date,promo_indicator,promo_start_date,promo_type,purchase_date,purchase_price,rate_cap,rate_floor,refinance_indicator,region,repayment_method,sale_amount,sale_date,sale_reason,scheduled_outstanding_principal_bop,scheduled_outstanding_principal_eop,secured_loan_indicator,service_fee_rate,servicing_fee,sold_to,state,sub_product_type,term_denomination,total_investor_fees,total_number_of_failed_payments,total_number_of_modifications,total_number_of_payments_missed,total_number_of_successful_payments,vintage,months_since_missed_payment,factor,loan_status_description,prior_loan_status,prior_loan_status_description,roll_bucket,fico_policy,fico_policy_description,model_price,model_spread,model_yield,model_principal,model_cumulative_principal,model_cumulative_interest,model_cumulative_charge_off,model_cumulative_recoveries,model_cumulative_loss,model_cumulative_servicing,model_cumulative_scheduled,fico_band,original_balance_band,coupon_band,vintage_band,never_late_indicator,originator_grade,originator_term,modified_duration,net_annualized_return,cumulative_nar,first_record,funded_amount,funded_amount_by_investors,fico_range_high,fico_range_high_origination,fico_range_low,fico_range_low_origination,job_title,outstanding_principal_funded_by_investors,outstanding_total_fee_balance,total_payments_received_by_investors,last_modification_date,last_modification_type,latest_promo_expiration_date,latest_promo_type,monthly_payment,note_day,note_principal,number_of_investors,onetwenty_dpd_accounts_current,onetwenty_dpd_accounts_ever,percentage_of_loan,total_service_fee,originator_custom_field_1,originator_custom_field_2,originator_custom_field_3,originator_custom_field_4,originator_custom_field_5,term,ach_flag,outstanding_ext_fee_balance,as_of_date,charge_off_reason_code,charge_off_reversal_date,coupon,extension_date,account_id,original_principal,outstanding_late_fee_balance,outstanding_misc_fee_balance,outstanding_nsf_fee_balance,outstanding_phone_pay_fee_balance,payment_plan_flag,refinance_flag,purchase_premium,maturity_date,loan_type,fico_current,fico_origination,etl_run_id,borrower_type,co_signer_flag,origination_channel,settlement_amount,post_settlement_charge_off_amount,broken_settlement_indicator,settlement_payment,settlement_effective_date,settlement_end_date,first_record_current_owner,first_record_loan_status,would_be_underwritten_today,free_cash_flow,monthly_housing_payment,forbearance_indicator,forbearance_type,forbearance_term,forbearance_start_date,forbearance_end_date,forbearance_age,forbearance_remaining_term,forbearance_interest_accrual_indicator,forbearance_accrued_interest,originator )\r\n                 FROM 's3://peeriq-emr-test/tables/rollup_tables/processedloanborrower_morganstanley_sofi_daily/d58ff837-bc47-4748-ac7f-418100223398/'\r\n                 NULL as '' DELIMITER '|' IGNOREHEADER as 1 CSV IAM_ROLE 'arn:aws:iam::380796340905:role/RedshiftCopyUnload'\r\n                 GZIP TRUNCATECOLUMNS COMPUPDATE OFF STATUPDATE OFF\r\n[2020-05-28 13:08:50] [XX000][500310] [Amazon](500310) Invalid operation: S3ServiceException:Access Denied,Status 403,Error AccessDenied,Rid ED41B9070CE8C4F0,ExtRid xdW5/76y/f/Xl7NmAyoQBmw7UiWNAy/zbP5zD4xZzxx4WQG5nbr0lE69kV2oHx4YGVjqwkHtQ8Y=,CanRetry 1\r\n[2020-05-28 13:08:50] Details:\r\n[2020-05-28 13:08:50] -----------------------------------------------\r\n[2020-05-28 13:08:50] error:  S3ServiceException:Access Denied,Status 403,Error AccessDenied,Rid ED41B9070CE8C4F0,ExtRid xdW5/76y/f/Xl7NmAyoQBmw7UiWNAy/zbP5zD4xZzxx4WQG5nbr0lE69kV2oHx4YGVjqwkHtQ8Y=,CanRetry 1\r\n[2020-05-28 13:08:50] code:      8001\r\n[2020-05-28 13:08:50] context:   S3 key being read : s3://peeriq-emr-test/tables/rollup_tables/processedloanborrower_morganstanley_sofi_daily/d58ff837-bc47-4748-ac7f-418100223398/part-00021-f3ea50d7-2e08-4862-9fd7-c4fcc7ad62c9-c000.csv.gz\r\n[2020-05-28 13:08:50] query:     12558648\r\n[2020-05-28 13:08:50] location:  copy_s3_scanner.cpp:261\r\n[2020-05-28 13:08:50] process:   query4_245_12558648 [pid=30314]\r\n[2020-05-28 13:08:50] -----------------------------------------------;\r\n```\r\nthat s3 key does exist - this was working fine beginning of this week too\r\n","created":"2020-05-28T17:13:51.000Z"},{"body":"@amitoj9 is this something to do with the MorganStanley pgp testing request? ","created":"2020-05-28T18:25:15.000Z"},{"author":"Barry Ickow","body":"nope @jojoms711 ","created":"2020-05-28T18:44:00.000Z"},{"body":"@barryPIQ  can you find AWS key that we are using in taskmanager","created":"2020-05-28T21:25:17.000Z"},{"author":"Barry Ickow","body":"just discussed, see prior comment @amitoj9 ","created":"2020-05-28T21:34:25.000Z"},{"body":"@barryPIQ  can you share AWS access key that is being used over here I don't  see it in the comments. \r\nIf we are pulling it from Vault can you share that too would be helpful.","created":"2020-05-29T14:08:56.000Z"},{"author":"Barry Ickow","body":"@amitoj9 last we discussed you agreed there is no s3 key involved - I'm not sure what key I would pull even\r\n\r\nsee the query below - that's whats generating the error - there is no s3 key used - just the redshift iam role as discussed","created":"2020-05-29T14:28:49.000Z"},{"body":"@barryPIQ there is nothing as such s3 key and for IAM role for redshift I need to know what user is assigned to your application, that is what I am asking over here. Access Keys are for users so if you share that it would be helpful.","created":"2020-06-04T15:32:34.000Z"},{"author":"Barry Ickow","body":"@amitoj9 just run the redshift query I posted please - we just need that to run, even on your local - again, that command doesn't have S3 keys and it worked fine a couple weeks ago before we posted the issue\r\n\r\nhttps://github.com/peeriq/aqueduct/issues/1851#issuecomment-635481508","created":"2020-06-04T16:06:32.000Z"},{"body":"moving to sprint 2 for june -- don't think we solve this yet and i believe @amitoj9 has some issue with testing this? do you need me to reproduce this error ? \r\n@amitoj9 @barryPIQ ","created":"2020-06-16T19:08:40.000Z"},{"body":"@jojoms711 I wasn't able to find out what was causing this problem, will further investigate it.","created":"2020-06-16T19:12:34.000Z"},{"body":"ok thanks - let me know if you need a fresh error.","created":"2020-06-16T19:19:48.000Z"}]},{"summary":"MDS DB and Cache Researching + Enhancements, mainly focused on load_metadata","externalId":"1795","repo":"aqueduct","description":"There are a few very related questions around the MDS DB and Cache, mainly relating to load_metadata:\r\n\r\n1) Do the load_metadata DB updates get committed all at once only after the whole function reaches completion? This might be a general django question unless load_metadata is doing very non-standard things, which is possible too of course.\r\n\r\n2) Does django automatically clear the cache once the DB is updated, and if so does this hold for load_metadata?  If not, do we have a cache class or something similar that handles that (https://github.com/peeriq/aqueduct/issues/1790)?\r\n\r\n3) Can load_metadata be optimized to run faster and ideally simpler (https://github.com/peeriq/aqueduct/issues/1690)?","created":"2020-04-06T21:36:34.000Z","issueType":"Task","reporter":"Barry Ickow","status":"TO DO","labels":["[zube]: Backlog","jira"],"comments":[]},{"summary":"(prod issue) EMR cluster is running but workers are not spinning up containers","externalId":"1777","repo":"aqueduct","description":"![image.png](https://zube.io/files/peeriq/7eee9cf34138d35243fcec6ad2c19ed3-image.png)\r\n\r\nThis was happening on April 3 (late night) and continues to April 4 (morning). EMR jobs are failing. \r\nCluster status: Running\r\n\r\n","created":"2020-04-03T17:13:46.000Z","issueType":"Task","status":"TO DO","labels":["[zube]: Backlog","service: ExtractLettuceTomato","jira"],"comments":[{"body":"Solution: Redeploy new EMR cluster\r\n\r\nReason for issue: AWS related ..\r\n\r\n@amitoj9 @mkchuprin - can you expand on the reason (one or 2 lines will do)","created":"2020-04-03T17:14:22.000Z"}]},{"summary":"Investigation - Can we speed up Circle CI testing time for aqueduct?","externalId":"1760","repo":"aqueduct","description":"Every PR in this repo takes at least 40 min to complete the circle CI test, I am not looking to skip any of these tests, but to understand if there is a way to speed up the tests from start to finish.  \r\n\r\n![image.png](https://zube.io/files/peeriq/955237c27beb2e4491939c90ecb5c755-image.png)\r\n\r\n@amitoj9 - i am assigning to you but you can assign it to @mkchuprin or somebody else for prioritization.  It is not a blocker, just thinking from an optimization perspective. ","created":"2020-03-31T13:53:47.000Z","issueType":"Task","status":"TO DO","labels":["[zube]: Backlog","Improvement","jira"],"comments":[{"body":"When we get to this issue, we should investigate this CircleCI option:\r\n\r\n![image](https://user-images.githubusercontent.com/2336431/78255311-2072df80-74c5-11ea-9189-8dab6e9f1792.png)\r\n","created":"2020-04-02T13:34:23.000Z"},{"body":"Which part of that is the option you are referring to? the queued builds or skipping redundant builds? I think the problem is we don't know which part is redundant? (and precisely we let it tests everything in case unknowingly our code may be touching some other areas)","created":"2020-04-02T13:50:48.000Z"},{"body":"I have to look into it more, but I think we can skip some redundant building. The job that takes 27:18 spends most of its time building. If we can cache the containers and only rebuild them when they change, then that would speed things up. ","created":"2020-04-02T13:53:55.000Z"},{"author":"Barry Ickow","body":"https://github.com/peeriq/aqueduct/issues/1805 will help too","created":"2020-04-10T05:05:29.000Z"},{"body":"Option: Use the CirclCI `resources` class to shift `ram` and `cpu` power to more needy jobs\r\nOption: Don't compile cython everytime, push the compiled files. Only compile if they are missing.  ","created":"2020-04-15T12:55:11.000Z"}]},{"summary":"Update desired cores to 2000 in Aqueduct Job Orchestrator","externalId":"1671","repo":"aqueduct","description":"Instead of submitting desired cored to 3000 update it to 2000 and see if the machines are still able to run the functionality or not\r\n\r\nLogs: \r\n``` 'arn:aws:batch:us-east-1:244069461843:compute-environment/data-transformer-prod-3' desired cores to 3000\r\n\r\n2020-03-02 17:17:08,202 - __main__ - 894 - INFO - Submit batch job {'ResponseMetadata': \r\n\r\n{\r\n'RequestId': '09848414-87ed-4309-af31-de18acbb7e33',\r\n 'HTTPStatusCode': 200, \r\n'HTTPHeaders':\r\n {\r\n'date': 'Mon, \r\n02 Mar 2020 17:17:08 GMT',\r\n 'content-type': 'application/json', \r\n'content-length': '107',\r\n 'connection': 'keep-alive',\r\n 'x-amzn-requestid': '09848414-87ed-4309-af31-de18acbb7e33',\r\n 'x-amz-apigw-id': 'IxbfKGuVoAMFaBw=',\r\n 'x-amzn-trace-id': 'Root=1-5e5d3f94-e0e1146aa46b71b28b909f7a;Sampled=0'\r\n},\r\n 'RetryAttempts': 0\r\n},\r\n 'jobName': '9a601076-c3ae-4002-bf66-3491accf7668-batch-job',\r\n 'jobId': 'ea8bfc45-3687-43b6-beed-a72bb55d87b1'}\r\n```\r\n\r\n\r\n","created":"2020-03-02T17:30:47.000Z","issueType":"Task","status":"TO DO","labels":["[zube]: Backlog","jira"],"comments":[{"body":"[Here](https://github.com/peeriq/aqueduct/pull/1673) is the code change to approve","created":"2020-03-02T17:46:50.000Z"},{"author":"Barry Ickow","body":"status @amitoj9 ?","created":"2020-04-15T14:54:08.000Z"},{"author":"Barry Ickow","body":"just add points as well, when you can, @amitoj9 ","created":"2020-04-15T14:54:43.000Z"}]}]}]}